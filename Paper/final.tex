% OKAY PLANNING TIME (sorry rose i dont know how to format the whole thing)


% ABSTRACT
% Description of experiment
% Results and conclusions

% INTRO
% - mostly copied from proposal
% - explain Genetic Algorithms (with pics?)
% - questions of interest


% METHODOLOGY
% - Explain the model
%  = Input/Output
%  = Layers (we can use http://alexlenail.me/NN-SVG/LeNet.html for the diagram)

% Input layer, 105 nodes
% Two Dense layers, 140 nodes, rectified linear unit activation.
% Output layer, 10 nodes, sigmoid activation function.

%  = Initial generation/weights

% Inital weights for every node were randomly generated using Python's built-in random.random() function.

% - Fitness calculation
%  = Grouping
%  = Playing the game
%  = Actual fitness formula
% - Mutation method
%  = Explain varAnd()
%  = (just so you know, we are using tools.cxTwoPoint as mating, and tools.mutFlipBit as mutation)



% RESULTS
% - Mean, max, min, median graphs
% - comparison between first, tenth, fiftyth and hundredth generation.
% - note random spikes



% DISCUSSION
% - For all intents and purposes, model failed to learn even basic Big2 moves.
% - Possible reasons:
%  = Power/Time issue:
%   \ Lack of computing power
%   \ Low generation count
%  = Model issue:
%   \ Model itself is flawed
%   \ Crossover or mutation does not do anything?
%  = Experiment issue:
%   \ Fitness calculation isn't descriptive enough.
%   \ Errors in programming?   
%  = Unfit-for-problem issue:
%   \ GAs are simply not suited for problem statement.
%   \ Too much randomness.
% - Possible improvements:


% BIBLIOGRAPHY
% Lets hit the GYM - https://kushalmukherjee.medium.com/lets-hit-the-gym-combining-neural-network-keras-with-genetic-algorithm-deap-371e962473c8 (ai code scaffolded from here)
% Big11 - https://github.com/kwccoin/big2-1/blob/master/Big11.py (big2 code taken from here)
% DEAP 1.3.3 Documentation - https://deap.readthedocs.io/en/master/index.html (GA section of the code taken here)
% 